% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/scrape-utils.R
\name{scrape}
\alias{scrape}
\title{Scrape}
\usage{
scrape(
  x,
  encoding = "",
  ...,
  options = c("RECOVER", "NOERROR", "NOBLANKS"),
  sleep_time = 3,
  verbose = TRUE
)
}
\arguments{
\item{x}{A string, a connection, or a raw vector.

A string can be either a path, a url or literal xml. Urls will
be converted into connections either using \code{base::url} or, if
installed, \code{curl::curl}. Local paths ending in \code{.gz},
\code{.bz2}, \code{.xz}, \code{.zip} will be automatically uncompressed.

If a connection, the complete connection is read into a raw vector before
being parsed.}

\item{encoding}{Specify a default encoding for the document. Unless
otherwise specified XML documents are assumed to be in UTF-8 or
UTF-16. If the document is not UTF-8/16, and lacks an explicit
encoding directive, this allows you to supply a default.}

\item{...}{Additional arguments passed on to methods.}

\item{options}{Set parsing options for the libxml2 parser. Zero or more of
\describe{
  \item{RECOVER}{recover on errors}
  \item{NOENT}{substitute entities}
  \item{DTDLOAD}{load the external subset}
  \item{DTDATTR}{default DTD attributes}
  \item{DTDVALID}{validate with the DTD}
  \item{NOERROR}{suppress error reports}
  \item{NOWARNING}{suppress warning reports}
  \item{PEDANTIC}{pedantic error reporting}
  \item{NOBLANKS}{remove blank nodes}
  \item{SAX1}{use the SAX1 interface internally}
  \item{XINCLUDE}{Implement XInclude substitition}
  \item{NONET}{Forbid network access}
  \item{NODICT}{Do not reuse the context dictionary}
  \item{NSCLEAN}{remove redundant namespaces declarations}
  \item{NOCDATA}{merge CDATA as text nodes}
  \item{NOXINCNODE}{do not generate XINCLUDE START/END nodes}
  \item{COMPACT}{compact small text nodes; no modification of the tree allowed afterwards (will possibly crash if you try to modify the tree)}
  \item{OLD10}{parse using XML-1.0 before update 5}
  \item{NOBASEFIX}{do not fixup XINCLUDE xml:base uris}
  \item{HUGE}{relax any hardcoded limit from the parser}
  \item{OLDSAX}{parse using SAX2 interface before 2.7.0}
  \item{IGNORE_ENC}{ignore internal document encoding hint}
  \item{BIG_LINES}{Store big lines numbers in text PSVI field}
}}

\item{sleep_time}{Time in seconds for the system to sleep before each scrape with \code{\link[xml2]{read_html}}.}

\item{verbose}{When reading from a slow connection, this prints some
output on every iteration so you know its working.}
}
\description{
Read html with a crawl delay set with the \code{sleep_time} argument. In addition, if the initial response returns NULL, the url is read again. A NULL will be returned if the second try also results in an error. All connections are closed before the response is returned.
}
\concept{scrape functions}
